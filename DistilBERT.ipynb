{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd25961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    DistilBERT training via knowledge distillation from BERT using PyTorch and Hugging Face Transformers.\n",
    "\"\"\"\n",
    "# !pip install torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    AdamW is a popular optimization algorithm in deep learning, especially well-suited for training models like Transformers (e.g., BERT, GPT, etc.). \n",
    "    It is a variant of the Adam optimizer that introduces a correct way to apply weight decay (L2 regularization).\n",
    "    AdamW helps prevent overfitting while maintaining the benefits of Adam (adaptive learning rates, momentum).\n",
    "    It is the default optimizer in Hugging Face Transformers and many other frameworks for fine-tuning pre-trained language models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Dataset\n",
    "dataset = load_dataset(\"imdb\", split='train[:1%]').train_test_split(test_size=0.2) # Loads 1% of the IMDb dataset's training set for a quick demo or test\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Loads the tokenizer that corresponds to bert-base-uncased\n",
    "\"\"\"\n",
    "    The IMDB dataset is a popular dataset used for binary sentiment classificationâ€”determining whether a movie review is positive or negative.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f3592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1ccefb5425457e8b8d703bd0ca1388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8ca8ff8cda4746ab605f9862988715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> first sample from dataset: \n",
      "{'label': tensor(0), 'input_ids': tensor([  101,  2023,  2003,  2009,  1012,  2023,  2003,  1996,  2028,  1012,\n",
      "         2023,  2003,  1996,  5409,  3185,  2412,  2081,  1012,  2412,  1012,\n",
      "         2009, 10299,  2673,  1012,  1045,  2031,  2196,  2464,  4788,  1012,\n",
      "        11036,  1996,  5384,  1998,  2507,  2009,  2000,  2122,  2111,  1012,\n",
      "         1012,  1012,  1012,  1012,  2045,  1005,  1055,  2074,  2053,  7831,\n",
      "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2130,\n",
      "         2093,  2420,  2044,  3666,  2023,  1006,  2005,  2070,  3114,  1045,\n",
      "         2145,  2123,  1005,  1056,  2113,  2339,  1007,  1045,  3685,  2903,\n",
      "         2129,  9577,  2135, 23512,  2023,  3185,  2003,  1013,  2001,  1012,\n",
      "         2049,  2061,  2919,  1012,  2061,  2521,  2013,  2505,  2008,  2071,\n",
      "         2022,  2641,  1037,  3185,  1010,  1037,  2466,  2030,  2505,  2008,\n",
      "         2323,  2031,  2412,  2042,  2580,  1998,  2716,  2046,  2256,  4598,\n",
      "         1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2023,\n",
      "         2081,  2033,  3160,  3251,  2030,  2025,  4286,  2024,  5621,  2404,\n",
      "         2006,  2023,  3011,  2000,  2079,  2204,  1012,  2009,  2081,  2033,\n",
      "         2514, 17733,  2007,  9731,  1998,  2256,  5082,  2004,  1037,  2427,\n",
      "         1999,  2023,  5304,  1012,  2023,  2828,  1997,  3185, 25664, 13403,\n",
      "         2149,  2004,  1037,  2554,  1012,  2057,  2323,  2022, 14984,  1012,\n",
      "         1045,  2428,  3685, 17902,  2008,  2256,  3795,  5368,  2004,  2111,\n",
      "         2542,  2182,  1998,  4526,  2396,  1010,  2003,  2008,  2057,  2342,\n",
      "         2000,  4652,  1996,  4325,  1997,  2122,  7977, 20870,  2015,  1997,\n",
      "         2256,  4507,  2005,  2256,  2219,  2204,  1012,  2009,  1005,  1055,\n",
      "         2019, 14325,  1012,  1045,  2123,  1005,  1056,  2113,  2129,  2006,\n",
      "         3011,  2151,  1997,  2122,  5889,  1010,  4898,  1010,  2030,  1996,\n",
      "         2472,  1997,  2023,  2143, 25126,  2012,  2305,  4209,  2008,  2027,\n",
      "         2018,  1037,  2535,  1999,  2437,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      ">>> shape of input_ids:  torch.Size([256])\n",
      ">>> shape of attention_mask:  torch.Size([256])\n",
      ">>> shape of label:  torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the Text\n",
    "def encode_batch(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\"\"\"\n",
    "    encode_batch(): Tokenizes batches of texts, with:\n",
    "        - truncation: Cuts long reviews down to max_length.\n",
    "        - padding='max_length': Ensures uniform tensor sizes.\n",
    "        - max_length=256: Keeps sequences to 256 tokens max.\n",
    "\"\"\"\n",
    "\n",
    "dataset = dataset.map(encode_batch, batched=True) # Applies tokenizer to all samples \n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label']) # Converts dataset to PyTorch tensors\n",
    "\"\"\"\n",
    "    It ensures that each time you access a sample from the dataset, it returns a dictionary like:\n",
    "        {\n",
    "            'input_ids': tensor(...),\n",
    "            'attention_mask': tensor(...),\n",
    "            'label': tensor(...)\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(\">>> first sample from dataset: \")\n",
    "print(dataset['train'][0])\n",
    "print(\">>> shape of input_ids: \", dataset['train'][0]['input_ids'].shape)\n",
    "print(\">>> shape of attention_mask: \", dataset['train'][0]['attention_mask'].shape)\n",
    "print(\">>> shape of label: \", dataset['train'][0]['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a0e7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> shape of input_ids in first_batch:  torch.Size([8, 256])\n",
      ">>> shape of attention_mask in first_batch:  torch.Size([8, 256])\n",
      ">>> shape of label in first_batch:  torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Apply DataLoader\n",
    "train_loader = DataLoader(dataset['train'], batch_size=8, shuffle=True) # Prepares data loader for batching\n",
    "first_batch = next(iter(train_loader)) # Get first batch\n",
    "\n",
    "print(\">>> shape of input_ids in first_batch: \", first_batch['input_ids'].shape)\n",
    "print(\">>> shape of attention_mask in first_batch: \", first_batch['attention_mask'].shape)\n",
    "print(\">>> shape of label in first_batch: \", first_batch['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f8f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
