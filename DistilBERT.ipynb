{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd25961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    DistilBERT training via knowledge distillation from BERT using PyTorch and Hugging Face Transformers.\n",
    "\"\"\"\n",
    "# !pip install torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, StepLR\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    AdamW is a popular optimization algorithm in deep learning, especially well-suited for training models like Transformers (e.g., BERT, GPT, etc.). \n",
    "    It is a variant of the Adam optimizer that introduces a correct way to apply weight decay (L2 regularization).\n",
    "    AdamW helps prevent overfitting while maintaining the benefits of Adam (adaptive learning rates, momentum).\n",
    "    It is the default optimizer in Hugging Face Transformers and many other frameworks for fine-tuning pre-trained language models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Dataset\n",
    "imdb_dataset = load_dataset(\"imdb\", split='train')\n",
    "imdb_dataset_shuffle = imdb_dataset.shuffle(seed=42)  # Shuffle the full train split\n",
    "dataset = imdb_dataset_shuffle.select(range(int(0.1 * len(imdb_dataset_shuffle))))  # Take 10% of the shuffled dataset randomly\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Loads the tokenizer that corresponds to bert-base-uncased\n",
    "\"\"\"\n",
    "    The IMDB dataset is a popular dataset used for binary sentiment classificationâ€”determining whether a movie review is positive or negative.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the Text\n",
    "def encode_batch(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\"\"\"\n",
    "    encode_batch(): Tokenizes batches of texts, with:\n",
    "        - truncation: Cuts long reviews down to max_length.\n",
    "        - padding='max_length': Ensures uniform tensor sizes.\n",
    "        - max_length=256: Keeps sequences to 256 tokens max.\n",
    "\"\"\"\n",
    "\n",
    "dataset = dataset.map(encode_batch, batched=True) # Applies tokenizer to all samples \n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label']) # Converts dataset to PyTorch tensors\n",
    "\"\"\"\n",
    "    It ensures that each time you access a sample from the dataset, it returns a dictionary like:\n",
    "        {\n",
    "            'input_ids': tensor(...),\n",
    "            'attention_mask': tensor(...),\n",
    "            'label': tensor(...)\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(\">>> length of dataset: \", len(dataset['train']))\n",
    "print(\">>> labels of dataset: \", dataset['train']['label']) # Check if the dataset slice is somehow balanced \n",
    "\n",
    "print(\">>> first sample from dataset: \")\n",
    "print(dataset['train'][0])\n",
    "print(\">>> shape of input_ids: \", dataset['train'][0]['input_ids'].shape)\n",
    "print(\">>> shape of attention_mask: \", dataset['train'][0]['attention_mask'].shape)\n",
    "print(\">>> shape of label: \", dataset['train'][0]['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DataLoader\n",
    "train_loader = DataLoader(dataset['train'], batch_size=8, shuffle=True) # Prepares data loader for batching\n",
    "print(\">>> length of train_loader: \", len(train_loader))\n",
    "\n",
    "first_batch = next(iter(train_loader)) # Get first batch\n",
    "print(\">>> shape of input_ids in first_batch: \", first_batch['input_ids'].shape)\n",
    "print(\">>> shape of attention_mask in first_batch: \", first_batch['attention_mask'].shape)\n",
    "print(\">>> shape of label in first_batch: \", first_batch['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Teacher and Student Models\n",
    "teacher_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) # Full BERT model fine-tuned for sequence classification\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2) # Lighter, faster DistilBERT model also for classification\n",
    "\n",
    "teacher_model.eval() # Freezes teacher_model for inference only\n",
    "student_model.train() # Prepares student_model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Distillation Loss\n",
    "class DistillationLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature=2.0, alpha=0.5):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, true_labels):\n",
    "        eps = 1e-7  # small epsilon to prevent log(0)\n",
    "\n",
    "        # Soft targets: distillation loss\n",
    "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        soft_teacher = torch.clamp(soft_teacher, min=eps, max=1.0) # Clamp to avoid log(0) -> -inf\n",
    "        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        kd_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (self.temperature ** 2)\n",
    "        \"\"\"\n",
    "            F.kl_div(input, target) expects:\n",
    "                - input: log-probabilities (i.e., log_softmax)\n",
    "                - target: probabilities (i.e., softmax)\n",
    "            If both are given as softmax, the KL loss can go negative or become numerically unstable.\n",
    "        \"\"\"\n",
    "\n",
    "        # Hard targets: standard classification loss\n",
    "        ce_loss = self.ce_loss(student_logits, true_labels)\n",
    "\n",
    "        # Total loss\n",
    "        return self.alpha * kd_loss + (1. - self.alpha) * ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da568bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Optimizer, Distillation Loss and Learning Rate Scheduler\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-4)\n",
    "kd_loss_fn = DistillationLoss(temperature=2.0, alpha=0.5)\n",
    "\n",
    "# Warm-up scheduler (over 4 epochs)\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=5)\n",
    "# After warm-up, switch to StepLR\n",
    "main_scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# Combine warmup_scheduler with main_scheduler\n",
    "scheduler = SequentialLR(optimizer,\n",
    "                         schedulers=[warmup_scheduler, main_scheduler],\n",
    "                         milestones=[5] # switch from warm-up to StepLR after 5 epochs\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722970d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA available: Yes\")\n",
    "    print(f\"Total GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\n--- GPU {i} ---\")\n",
    "        print(f\"Name: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(i)/1024**2:.2f} MB\")\n",
    "        print(f\"Memory Reserved: {torch.cuda.memory_reserved(i)/1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"CUDA available: No. Using CPU.\")\n",
    "\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697395ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with Knowledge Distillation\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device) # torch.Size([8, 256])\n",
    "        attention_mask = batch['attention_mask'].to(device) # torch.Size([8, 256])\n",
    "        labels = batch['label'].to(device) # torch.Size([8])\n",
    "    \n",
    "        with torch.no_grad(): # Runs the teacher model in inference mode (no gradients computed)\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits \n",
    "            \"\"\"\n",
    "                For BERT, the return value (teacher_logits) is specifically a SequenceClassifierOutput object:\n",
    "\n",
    "                    SequenceClassifierOutput(\n",
    "                        loss=None,  # only present if labels are passed\n",
    "                        logits=tensor(...),\n",
    "                        hidden_states=None,  # optional\n",
    "                        attentions=None      # optional\n",
    "                    )\n",
    "            \"\"\"\n",
    "\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits # Shape: [batch_size, num_classes] -> torch.Size([8, 2])\n",
    "\n",
    "        loss = kd_loss_fn(student_logits, teacher_logits, labels)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss is NaN! Debug info:\")\n",
    "            print(\"Student logits:\", student_logits)\n",
    "            print(\"Teacher logits:\", teacher_logits)\n",
    "            print(\"Labels:\", labels)\n",
    "            break \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[-1]['lr']\n",
    "    print(f\">>> Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, LR: {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5310b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DistilBERT and BERT models\n",
    "torch.save(student_model.state_dict(), 'DistilBERT_model_weights.pth')\n",
    "torch.save(teacher_model.state_dict(), 'BERT_base_model_weights.pth')\n",
    "\n",
    "# Load the state_dict\n",
    "student_model.load_state_dict(torch.load('DistilBERT_model_weights.pth'))\n",
    "teacher_model.load_state_dict(torch.load('BERT_base_model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loop for Evaluation\n",
    "test_loader = DataLoader(dataset['test'], batch_size=4) # Prepares data loader for batching\n",
    "print(\">>> length of test_loader: \", len(test_loader))\n",
    "\n",
    "student_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad(): # No need to compute gradients during evaluation\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device) # torch.Size([4, 256])\n",
    "        attention_mask = batch['attention_mask'].to(device) # torch.Size([4, 256])\n",
    "        labels = batch['label'].to(device) # torch.Size([4])\n",
    "\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "        predictions = torch.argmax(student_logits, dim=1)\n",
    "\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        all_preds.extend(predictions.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\">>> Test Accuracy of DistilBERT: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of parameters for DistilBERT\n",
    "total_params = sum(p.numel() for p in student_model.parameters())\n",
    "print(f\">>> Total parameters of DistilBERT: {total_params}\")\n",
    "# Total number of trainable parameters for DistilBERT\n",
    "trainable_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
    "print(f\">>> Trainable parameters of DistilBERT: {trainable_params}\")\n",
    "# Model's file size of DistilBERT\n",
    "file_size = os.path.getsize(\"DistilBERT_model_weights.pth\") / (1024 ** 2) # size in MB\n",
    "print(f\">>> Model file size of DistilBERT: {file_size:.2f} MB\")\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "# Total number of parameters for BERT\n",
    "total_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(f\">>> Total parameters of BERT: {total_params}\")\n",
    "# Total number of trainable parameters for BERT\n",
    "trainable_params = sum(p.numel() for p in teacher_model.parameters() if p.requires_grad)\n",
    "print(f\">>> Trainable parameters of BERT: {trainable_params}\")\n",
    "# Model's file size of BERT\n",
    "file_size = os.path.getsize(\"BERT_base_model_weights.pth\") / (1024 ** 2) # size in MB\n",
    "print(f\">>> Model file size of BERT: {file_size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
