{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd25961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    DistilBERT training via knowledge distillation from BERT using PyTorch and Hugging Face Transformers.\n",
    "\"\"\"\n",
    "# !pip install torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9f7c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    AdamW is a popular optimization algorithm in deep learning, especially well-suited for training models like Transformers (e.g., BERT, GPT, etc.). \n",
    "    It is a variant of the Adam optimizer that introduces a correct way to apply weight decay (L2 regularization).\n",
    "    AdamW helps prevent overfitting while maintaining the benefits of Adam (adaptive learning rates, momentum).\n",
    "    It is the default optimizer in Hugging Face Transformers and many other frameworks for fine-tuning pre-trained language models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Dataset\n",
    "imdb_dataset = load_dataset(\"imdb\", split='train')\n",
    "imdb_dataset_shuffle = imdb_dataset.shuffle(seed=42)  # Shuffle the full train split\n",
    "dataset = imdb_dataset_shuffle.select(range(int(0.02 * len(imdb_dataset_shuffle))))  # Take 2% of the shuffled dataset randomly\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Loads the tokenizer that corresponds to bert-base-uncased\n",
    "\"\"\"\n",
    "    The IMDB dataset is a popular dataset used for binary sentiment classificationâ€”determining whether a movie review is positive or negative.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "636f3592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8672575853459da153f98578fca724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e69737c01748388f3fa5546cc79b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> length of dataset:  400\n",
      ">>> labels of dataset:  tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0])\n",
      ">>> first sample from dataset: \n",
      "{'label': tensor(1), 'input_ids': tensor([  101,  1996, 10430,  2015,  2003,  2763,  1996,  2087,  4235, 10251,\n",
      "         2694,  2186,  2412,  1010,  2061,  8100,  2026, 10908,  2020,  2083,\n",
      "         1996,  4412,  1010,  1998,  2664,  1996,  2265, 15602,  2068,  1012,\n",
      "         1045,  2293,  1996, 13897,  1998,  4126,  6907,  1999,  2143,  1998,\n",
      "         1045,  5959,  2206,  1996, 17075,  3441,  2275,  1999,  2122,  8484,\n",
      "         1010,  2021,  2023,  2003,  2061,  2172,  2062,  1012,  6564,  1009,\n",
      "         2847,  1997,  3430,  3957,  1996,  2466,  1037,  3382,  2000,  2025,\n",
      "         2069,  2022,  2028,  1997,  1996,  2087, 26162,  1998, 21446, 13897,\n",
      "         1013,  2895,  3441,  1010,  2021,  2036,  2000,  2022,  1037,  2307,\n",
      "         2155,  3689,  1010,  1037, 16880,  2839,  2817,  1010,  1037,  4756,\n",
      "         1011,  2041,  1011,  5189,  4038,  1010,  1037,  8235,  8317,  7749,\n",
      "         7149,  2007,  1996,  3267,  1997,  2204,  1998,  4763,  1010,  1998,\n",
      "         2019,  7789,  2396,  2100,  5792,  1997,  4387,  5544,  1998,  2534,\n",
      "        14194, 22045,  2035,  1999,  2028,  1012,  2585,  5252,  1005,  1055,\n",
      "         8680,  2186,  9020,  2000, 14570,  2035,  1997,  2023,  1998,  2062,\n",
      "         1010,  1998, 11297,  2015, 14633,  2004,  1996,  7541,  2694,  2064,\n",
      "         2131,  2000, 21014, 15401,  1010, 28007,  1996,  2346,  2005,  1037,\n",
      "         2193,  1997,  2060,  2186,  2000,  3613, 11221,  9501,  2185,  1012,\n",
      "         1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028, 15650,  2003,\n",
      "         2556,  2043,  2009,  2003,  2734,  1010,  2021,  5252,  1005,  1055,\n",
      "         6567,  2000, 18280,  2013,  2009,  2005,  3466,  2006,  6686,  2005,\n",
      "         1000,  3959,  4178,  1000,  1998,  1996,  2066,  2069,  9909,  2062,\n",
      "         9014,  2000,  1996,  2186,  1012,  5252,  1011,  1011,  2247,  2007,\n",
      "         1037,  2844,  3015,  3095,  2164,  5487, 11417,  3678,  1998, 25170,\n",
      "         5897,  3467,  1010,  2925, 17277,  1997,  5506,  2273,  1998, 29496,\n",
      "         3400,  4414,  1011,  1011,  4332,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      ">>> shape of input_ids:  torch.Size([256])\n",
      ">>> shape of attention_mask:  torch.Size([256])\n",
      ">>> shape of label:  torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the Text\n",
    "def encode_batch(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\"\"\"\n",
    "    encode_batch(): Tokenizes batches of texts, with:\n",
    "        - truncation: Cuts long reviews down to max_length.\n",
    "        - padding='max_length': Ensures uniform tensor sizes.\n",
    "        - max_length=256: Keeps sequences to 256 tokens max.\n",
    "\"\"\"\n",
    "\n",
    "dataset = dataset.map(encode_batch, batched=True) # Applies tokenizer to all samples \n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label']) # Converts dataset to PyTorch tensors\n",
    "\"\"\"\n",
    "    It ensures that each time you access a sample from the dataset, it returns a dictionary like:\n",
    "        {\n",
    "            'input_ids': tensor(...),\n",
    "            'attention_mask': tensor(...),\n",
    "            'label': tensor(...)\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(\">>> length of dataset: \", len(dataset['train']))\n",
    "print(\">>> labels of dataset: \", dataset['train']['label'])\n",
    "\n",
    "print(\">>> first sample from dataset: \")\n",
    "print(dataset['train'][0])\n",
    "print(\">>> shape of input_ids: \", dataset['train'][0]['input_ids'].shape)\n",
    "print(\">>> shape of attention_mask: \", dataset['train'][0]['attention_mask'].shape)\n",
    "print(\">>> shape of label: \", dataset['train'][0]['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7a0e7e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> length of train_loader:  50\n",
      ">>> shape of input_ids in first_batch:  torch.Size([8, 256])\n",
      ">>> shape of attention_mask in first_batch:  torch.Size([8, 256])\n",
      ">>> shape of label in first_batch:  torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Apply DataLoader\n",
    "train_loader = DataLoader(dataset['train'], batch_size=8, shuffle=True) # Prepares data loader for batching\n",
    "print(\">>> length of train_loader: \", len(train_loader))\n",
    "\n",
    "first_batch = next(iter(train_loader)) # Get first batch\n",
    "print(\">>> shape of input_ids in first_batch: \", first_batch['input_ids'].shape)\n",
    "print(\">>> shape of attention_mask in first_batch: \", first_batch['attention_mask'].shape)\n",
    "print(\">>> shape of label in first_batch: \", first_batch['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Teacher and Student Models\n",
    "teacher_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) # Full BERT model fine-tuned for sequence classification\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2) # Lighter, faster DistilBERT model also for classification\n",
    "\n",
    "teacher_model.eval() # Freezes teacher_model for inference only\n",
    "student_model.train() # Prepares student_model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5fc9b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Distillation Loss\n",
    "class DistillationLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature=2.0, alpha=0.5):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, true_labels):\n",
    "        eps = 1e-7  # small epsilon to prevent log(0)\n",
    "\n",
    "        # Soft targets: distillation loss\n",
    "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        soft_teacher = torch.clamp(soft_teacher, min=eps, max=1.0) # Clamp to avoid log(0) -> -inf\n",
    "        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        kd_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (self.temperature ** 2)\n",
    "        \"\"\"\n",
    "            F.kl_div(input, target) expects:\n",
    "                - input: log-probabilities (i.e., log_softmax)\n",
    "                - target: probabilities (i.e., softmax)\n",
    "            If both are given as softmax, the KL loss can go negative or become numerically unstable.\n",
    "        \"\"\"\n",
    "\n",
    "        # Hard targets: standard classification loss\n",
    "        ce_loss = self.ce_loss(student_logits, true_labels)\n",
    "\n",
    "        # Total loss\n",
    "        return self.alpha * kd_loss + (1. - self.alpha) * ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da568bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Optimizer and Distillation Loss\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n",
    "kd_loss_fn = DistillationLoss(temperature=2.0, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "722970d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: Yes\n",
      "Total GPUs: 1\n",
      "\n",
      "--- GPU 0 ---\n",
      "Name: NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "Capability: (8, 9)\n",
      "Memory Allocated: 1055.77 MB\n",
      "Memory Reserved: 3620.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA available: Yes\")\n",
    "    print(f\"Total GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\n--- GPU {i} ---\")\n",
    "        print(f\"Name: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(i)/1024**2:.2f} MB\")\n",
    "        print(f\"Memory Reserved: {torch.cuda.memory_reserved(i)/1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"CUDA available: No. Using CPU.\")\n",
    "\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "697395ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:14<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 1, Loss: 0.3520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 2, Loss: 0.3068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 3, Loss: 0.2654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 4, Loss: 0.2430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:14<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 5, Loss: 0.2356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 6, Loss: 0.2317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 7, Loss: 0.2311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 8, Loss: 0.2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 9, Loss: 0.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:14<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 10, Loss: 0.2293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 11, Loss: 0.2285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 12, Loss: 0.2283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with Knowledge Distillation\n",
    "epochs = 12\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device) # torch.Size([8, 256])\n",
    "        attention_mask = batch['attention_mask'].to(device) # torch.Size([8, 256])\n",
    "        labels = batch['label'].to(device) # torch.Size([8])\n",
    "    \n",
    "        with torch.no_grad(): # Runs the teacher model in inference mode (no gradients computed)\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits \n",
    "            \"\"\"\n",
    "                For BERT, the return value (teacher_logits) is specifically a SequenceClassifierOutput object:\n",
    "\n",
    "                    SequenceClassifierOutput(\n",
    "                        loss=None,  # only present if labels are passed\n",
    "                        logits=tensor(...),\n",
    "                        hidden_states=None,  # optional\n",
    "                        attentions=None      # optional\n",
    "                    )\n",
    "            \"\"\"\n",
    "\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        loss = kd_loss_fn(student_logits, teacher_logits, labels)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss is NaN! Debug info:\")\n",
    "            print(\"Student logits:\", student_logits)\n",
    "            print(\"Teacher logits:\", teacher_logits)\n",
    "            print(\"Labels:\", labels)\n",
    "            break \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\">>> Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
